<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>K-近邻算法 | I See Fire</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">K-近邻算法</h1><a id="logo" href="/.">I See Fire</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首頁</i></a><a href="/archives/"><i class="icon-archive"> 所有文章</i></a><a href="/about/"><i class="icon-about"> 關於</i></a><a href="/atom.xml"><i class="icon-rss"> 訂閱</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">K-近邻算法</h1><div class="post-meta">2016-02-17 | <span class="categories">分類於<a href="/categories/machine-learning/"> machine learning</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目錄</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#u7EA6_u4F1A_u7F51_u7AD9_u914D_u5BF9_u6548_u679C_u6539_u8FDB"><span class="toc-number">1.</span> <span class="toc-text">约会网站配对效果改进</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#u95EE_u9898_u9610_u8FF0"><span class="toc-number">1.1.</span> <span class="toc-text">问题阐述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u51C6_u5907_u6570_u636E"><span class="toc-number">1.2.</span> <span class="toc-text">准备数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u5206_u6790_u6570_u636E_uFF1AMatplotlib"><span class="toc-number">1.3.</span> <span class="toc-text">分析数据：Matplotlib</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u51C6_u5907_u6570_u636E_uFF1A_u5F52_u4E00_u5316_u6570_u503C"><span class="toc-number">1.4.</span> <span class="toc-text">准备数据：归一化数值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-_u8FD1_u90BB_u5206_u7C7B_u5668"><span class="toc-number">1.5.</span> <span class="toc-text">K-近邻分类器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#u5206_u7C7B_u5668_u6D4B_u8BD5"><span class="toc-number">1.6.</span> <span class="toc-text">分类器测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u603B_u7ED3"><span class="toc-number">2.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><p>K-近邻算法是分类数据最简单有效的算法,一般遵循下述流程:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1)&#25910;&#38598;&#25968;&#25454;&#65306;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#26041;&#27861;&#10;2)&#20934;&#22791;&#25968;&#25454;&#65306;&#36317;&#31163;&#35745;&#31639;&#25152;&#38656;&#35201;&#30340;&#25968;&#20540;&#65292;&#26368;&#22909;&#26159;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#26684;&#24335;&#10;3)&#20998;&#26512;&#25968;&#25454;&#65306;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#26041;&#27861;&#10;4)&#35757;&#32451;&#31639;&#27861;&#65306;&#27492;&#27493;&#39588;&#19981;&#36866;&#29992;K-&#36817;&#37051;&#31639;&#27861;&#10;5)&#27979;&#35797;&#31639;&#27861;&#65306;&#35745;&#31639;&#38169;&#35823;&#29575;&#10;6)&#20351;&#29992;&#31639;&#27861;&#65306;&#39318;&#20808;&#38656;&#35201;&#36755;&#20837;&#26679;&#26412;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#32467;&#26524;&#65292;&#10;&#28982;&#21518;&#36816;&#34892;K-&#36817;&#37051;&#31639;&#27861;&#21028;&#23450;&#36755;&#20837;&#25968;&#25454;&#20998;&#21035;&#23646;&#20110;&#21738;&#20010;&#20998;&#31867;&#65292;&#10;&#26368;&#21518;&#24212;&#29992;&#23545;&#35745;&#31639;&#20986;&#30340;&#20998;&#31867;&#25191;&#34892;&#21518;&#32493;&#30340;&#22788;&#29702;</span><br></pre></td></tr></table></figure></p>
<h2 id="u7EA6_u4F1A_u7F51_u7AD9_u914D_u5BF9_u6548_u679C_u6539_u8FDB"><a href="#u7EA6_u4F1A_u7F51_u7AD9_u914D_u5BF9_u6548_u679C_u6539_u8FDB" class="headerlink" title="约会网站配对效果改进"></a>约会网站配对效果改进</h2><h3 id="u95EE_u9898_u9610_u8FF0"><a href="#u95EE_u9898_u9610_u8FF0" class="headerlink" title="问题阐述"></a>问题阐述</h3><p>海伦一直使用在线约会网站寻找自己适合的约会对象，她发现曾交往过三种类型的人：</p>
<ul>
<li>不喜欢的人</li>
<li>魅力一般的人</li>
<li>极具魅力的人</li>
</ul>
<p>海伦希望网站能够更好地帮助她将匹配对象划分到确切地分类。同时，她海收集了一些约会网站未曾记录地数据信息，认为这些数据有助于匹配对象归类。样本主要包含以下三种特征：</p>
<ul>
<li>每年获得地飞行常客里程数</li>
<li>玩视频游戏耗费时间比</li>
<li>每周消费地冰淇淋公升数</li>
</ul>
<p>样本数据存于文本文件，每个样本占据一行，共1000行，最后一列代表对象类型。</p>
<h3 id="u51C6_u5907_u6570_u636E"><a href="#u51C6_u5907_u6570_u636E" class="headerlink" title="准备数据"></a>准备数据</h3><p>将特征数据输入到分类器之前，必须满足分类器的输入格式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/* &#36755;&#20837;&#20026;&#25991;&#26412;&#65292;&#36755;&#20986;&#20026;&#35757;&#32451;&#26679;&#26412;&#30697;&#38453;&#21644;&#31867;&#26631;&#31614;&#21521;&#37327; */&#10;def file2matrix(filename):&#10;    fr = open(filename)&#10;    arrayOLines = fr.readlines()&#10;    numberOfLines = len(arrayOLines)&#10;    returnMat = zeros((numberOfLines,3))&#10;    classLabelVector = []&#10;    index = 0&#10;    for line in arrayOLines:&#10;        listFromLine = line.strip().split(&#39;\t&#39;)&#10;        returnMat[index, :] = listFromLine[0:3]&#10;        classLabelVector.append(int(listFromLine[-1]))&#10;        index += 1&#10;    return returnMat, classLabelVector</span><br></pre></td></tr></table></figure>
<h3 id="u5206_u6790_u6570_u636E_uFF1AMatplotlib"><a href="#u5206_u6790_u6570_u636E_uFF1AMatplotlib" class="headerlink" title="分析数据：Matplotlib"></a>分析数据：Matplotlib</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()&#10;ax = fig.add_subplot(111)&#10;ax.scatter(returnMat[:,0], returnMat[:,1],\&#10;    15.0*array(classLabelVector), 15.0*array(classLabelVector))&#10;plt.show()</span><br></pre></td></tr></table></figure>
<p>利用datingLabels存储的类标签属性，在散点上绘制了色彩不等，尺寸不同的点，这里选取后两个特征绘制散点图。<br><img src="http://7xqb0e.com1.z0.glb.clouddn.com/k1matplot.png" alt=""></p>
<h3 id="u51C6_u5907_u6570_u636E_uFF1A_u5F52_u4E00_u5316_u6570_u503C"><a href="#u51C6_u5907_u6570_u636E_uFF1A_u5F52_u4E00_u5316_u6570_u503C" class="headerlink" title="准备数据：归一化数值"></a>准备数据：归一化数值</h3><p>利用欧式距离进行计算，发现第一个特征对于最终结果的影响远大于其他特征，而海伦认为这3个特征是同等重要的。在处理这种不同取值范围的特征值时，通常采用归一化的方法，将取值范围转化至$[0,1]$之间，可利用如下公式:<br>$$newValue = \frac{oldValue - min}{max - min}$$<br>其中$min$和$max$分别为数据集中每一维特征的最小特征值和最大特征值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def autoNorm(dataSet):&#10;    minVals = dataSet.min(0)&#10;    maxVals = dataSet.max(0)&#10;    ranges = maxVals - minVals&#10;    normDataSet = zeros(shape(dataSet))&#10;    m = dataSet.shape[0]&#10;    normDataSet = dataSet - tile(minVals, (m,1))&#10;    normDataSet = normDataSet/tile(ranges,(m,1))&#10;    return normDataSet, ranges, minVals</span><br></pre></td></tr></table></figure>
<h3 id="K-_u8FD1_u90BB_u5206_u7C7B_u5668"><a href="#K-_u8FD1_u90BB_u5206_u7C7B_u5668" class="headerlink" title="K-近邻分类器"></a>K-近邻分类器</h3><p>计算完所有点的距离后，可以对数据按照从小到大的次序排序；确定前K个距离最小的元素所在的主要分类，最后将<br>返回前K个中发生频率最高的元素标签。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def classify0(inX, dataSet, labels, k):&#10;    dataSetSize = dataSet.shape[0]&#10;    diffMat = tile(inX, (dataSetSize,1)) - dataSet&#10;    sqDiffMat = diffMat**2&#10;    sqDistances = sqDiffMat.sum(axis=1)&#10;    distances = sqDistances**0.5&#10;    sortedDistIndicies = distances.argsort()&#10;    classCount = &#123;&#125;&#10;    for i in range(k):&#10;        voteIlabel = labels[sortedDistIndicies[i]]&#10;        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1&#10;    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)&#10;    return sortedClassCount[0][0]</span><br></pre></td></tr></table></figure>
<h3 id="u5206_u7C7B_u5668_u6D4B_u8BD5"><a href="#u5206_u7C7B_u5668_u6D4B_u8BD5" class="headerlink" title="分类器测试"></a>分类器测试</h3><p>对于分类器来说，错误类就是分类器给出错误结果的次数占所有测试数据总数的比例。错误类作为设计分类器的评价指标。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def datingClassTest():&#10;    hoRatio = 0.20&#10;    datingDataMat, datingLabels = file2matrix(&#39;datingTestSet2.txt&#39;)&#10;    normMat, ranges, minVals = autoNorm(datingDataMat)&#10;    m = normMat.shape[0]&#10;    numTestVecs = int(m*hoRatio)&#10;    errorCount = 0.0&#10;    for i in range(numTestVecs):&#10;        classifierResult = classify0(normMat[i,:], normMat[numTestVecs:m,:],datingLabels[numTestVecs:m],3)&#10;        print &#34;the classifier came back with: %d, the real answer is: %d&#34; %(classifierResult, datingLabels[i])&#10;        if(classifierResult != datingLabels[i]):&#10;            errorCount += 1.0&#10;    print &#39;the total error rate is: %f&#39; %(errorCount/numTestVecs)</span><br></pre></td></tr></table></figure>
<p>分类器处理约会数据集的错误率接近2.6%，由于样本数据局限，这个数据可能不是十分准确，只是个示例。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>K-近邻算法拥有精度高，对异常值不敏感和无数据输入假定等优点，但是计算复杂和空间复杂度高。K-近邻算法时基于实例的学习，使用算法时必须有接近实际数据的训练样本数据，存储这些训练样本需要使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用可能非常耗时。<br>另一个缺陷在于它无法给出任何数据的基础结构信息，因此无法知晓平均实例样本和典型实例样本具有什么特征。</p>
</div><div class="tags"><a href="/tags/machine-learning/">machine learning</a></div><div class="post-nav"><a href="/2016/02/01/enum/" class="next">策略枚举<i class="icon-next"></i></a></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title">分類</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工作/">工作</a></li></ul></div><div class="widget"><div class="widget-title">標籤</div><div class="tagcloud"><a href="/tags/ranker/" style="font-size: 15px;">ranker</a> <a href="/tags/预算/" style="font-size: 15px;">预算</a> <a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/enum/" style="font-size: 15px;">enum</a></div></div><div class="widget"><div class="widget-title">最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/02/17/knn/">K-近邻算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/enum/">策略枚举</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/26/ranker-online/">ADX 排序模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/20/magina/">实时预算分配</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/04/git/">Git 基本命令</a></li></ul></div><div class="widget"><div class="widget-title">友站連結</div><ul></ul><a href="http://www.joelonsoftware.com/" title="Joe On Software" target="_blank">Joe On Software</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">I See Fire.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>