<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>决策树 | I See Fire</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">决策树</h1><a id="logo" href="/.">I See Fire</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首頁</i></a><a href="/archives/"><i class="icon-archive"> 所有文章</i></a><a href="/about/"><i class="icon-about"> 關於</i></a><a href="/atom.xml"><i class="icon-rss"> 訂閱</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">决策树</h1><div class="post-meta">2016-02-19 | <span class="categories">分類於<a href="/categories/maching-learning/"> maching learning</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目錄</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#u4FE1_u606F_u589E_u76CA"><span class="toc-number">1.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u5212_u5206_u6570_u636E_u96C6"><span class="toc-number">2.</span> <span class="toc-text">划分数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811"><span class="toc-number">3.</span> <span class="toc-text">递归调用决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u603B_u7ED3"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><h2 id="u4FE1_u606F_u589E_u76CA"><a href="#u4FE1_u606F_u589E_u76CA" class="headerlink" title="信息增益"></a>信息增益</h2><p>划分数据集的最大原则是：将无序的数据变的更加有序。组织杂乱无章的数据的一种方法就是使用信息论度量信息。在划分数据集前后发生的信息变化称为信息增益，通过计算每个特征值划分数据集获的信息增益，获得信息增益最高的特征就是最优的选择。<br>如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为:<br>$$l(x_i) = -p(x<em>i)$$<br>而熵表示为所有类别可能值包含的信息期望值，定义如下:<br>$$H=-\sum</em>{i=1}^np(x_i) $$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def calcShannonEnt(dataSet):&#10;    numEntries = len(dataSet)&#10;    print numEntries&#10;    labelCounts = &#123;&#125;&#10;    for featVec in dataSet:&#10;        currentLabel = featVec[-1]&#10;        if currentLabel not in labelCounts:&#10;            labelCounts[currentLabel] = 0&#10;        labelCounts[currentLabel] += 1&#10;    shannonEnt = 0.0&#10;    print labelCounts&#10;    for key in labelCounts:&#10;        prob = float(labelCounts[key])/numEntries&#10;        shannonEnt -= prob * (log(prob)/log(2))&#10;    return shannonEnt</span><br></pre></td></tr></table></figure>
<h2 id="u5212_u5206_u6570_u636E_u96C6"><a href="#u5212_u5206_u6570_u636E_u96C6" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>分类算法除了需要测量信息熵，还需要划分数据集，度量数据集的熵，以便判断是否正确的划分了数据集。需要对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最优的选择。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def splitDataSet(dataSet, axis, value):&#10;    retDataSet = []&#10;    for featVec in dataSet:&#10;        if featVec[axis] == value:&#10;            reducedFeatVec = featVec[:axis]&#10;            reducedFeatVec.extend(featVec[axis+1:])&#10;            retDataSet.append(reducedFeatVec)&#10;    return retDataSet&#10;&#10;def chooseBestFeatureToSplit(dataSet):&#10;    numFeatures = len(dataSet[0])-1&#10;    baseEntropy = calcShannonEnt(dataSet)&#10;    bestInfoGain = 0.0&#10;    bestFeature = -1&#10;    for i in range(numFeatures):&#10;        featList = [example[i] for example in dataSet]&#10;        uniqueVals = set(featList)&#10;        newEntropy = 0.0&#10;        for value in uniqueVals:&#10;            subDataSet = splitDataSet(dataSet, i, value)&#10;            prob = len(subDataSet)/float(len(dataSet))&#10;            newEntropy += prob * calcShannonEnt(subDataSet)&#10;        infoGain = baseEntropy - newEntropy&#10;        print infoGain&#10;        if(infoGain &#62; bestInfoGain):&#10;            bestInfoGain = infoGain&#10;            bestFeature = i&#10;    return bestFeature</span><br></pre></td></tr></table></figure>
<h2 id="u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811"><a href="#u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811" class="headerlink" title="递归调用决策树"></a>递归调用决策树</h2><p>工作原理：得到原始数据集，基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，再次划分数据，因此可以用递归的原则建立决策树。递归终止条件满足:</p>
<ul>
<li>遍历数据集的所有特征属性</li>
<li>每个分支下所有的实例都具有相同的分类</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def majorityCnt(classList):&#10;    classCount = &#123;&#125;&#10;    for vote in classList:&#10;        if vote not in classCount:&#10;            classCount[vote] = 0&#10;        classCount[vote] += 1&#10;    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)&#10;    return sortedClassCount[0][0]&#10;&#10;def createTree(dataSet, labels):&#10;    classList = [example[-1] for example in dataSet]&#10;    //&#31867;&#21035;&#23436;&#20840;&#30456;&#21516;&#65292; &#20572;&#27490;&#32487;&#32493;&#21010;&#20998;&#10;    if classList.count(classList[0]) == len(classList):&#10;        return classList[0]&#10;    //&#36941;&#21382;&#23436;&#25152;&#26377;&#29305;&#24449;&#36820;&#22238;&#20986;&#29616;&#27425;&#25968;&#26368;&#22810;&#30340;&#10;    if len(dataSet[0]) == 1:&#10;        return majorityCnt(classList)&#10;    bestFeat = chooseBestFeatureToSplit(dataSet)&#10;    bestFeatLabel = labels[bestFeat]&#10;    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;&#10;    del(labels[bestFeat])&#10;    featValues = [example[bestFeat] for example in dataSet]&#10;    uniqueVals = set(featValues)&#10;    for value in uniqueVals:&#10;        subLabels = labels[:]&#10;        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)&#10;    return myTree&#10;``` &#10;&#10;## Matplotlib&#32472;&#21046;&#20915;&#31574;&#26641;</span><br></pre></td></tr></table></figure>
<p>import matplotlib.pyplot as plt</p>
<p>decisionNode = dict(boxstyle=”sawtooth”, fc=”0.8”)<br>leafNode = dict(boxstyle=”round4”, fc=”0.8”)<br>arrow_args = dict(arrowstyle=”&lt;-“)</p>
<p>def plotNode(nodeTxt, centerPt, parentPt, nodeType):<br>    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=’axes fraction’, \<br>        xytext=centerPt, textcoords=’axes fraction’, va=”center”, ha=”center”, \<br>        bbox=nodeType, arrowprops=arrow_args)</p>
<p>def plotMidText(cntrPt, parentPt, txtString):<br>    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]<br>    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]<br>    createPlot.ax1.text(xMid, yMid, txtString)</p>
<p>def plotTree(myTree, parentPt, nodeTxt):<br>    numLeafs = getNumLeafs(myTree)<br>    depth = getTreeDepth(myTree)<br>    firstStr = myTree.keys()[0]<br>    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)<br>    plotMidText(cntrPt, parentPt, nodeTxt)<br>    plotNode(firstStr, cntrPt, parentPt, decisionNode)<br>    secondDict = myTree[firstStr]<br>    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD<br>    for key in secondDict.keys():<br>        if type(secondDict[key]).<strong>name</strong> == ‘dict’:<br>            plotTree(secondDict[key], cntrPt, str(key))<br>        else:<br>            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW<br>            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)<br>            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))<br>    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD</p>
<p>def createPlot(inTree):<br>    fig = plt.figure(1, facecolor=’white’)<br>    fig.clf()<br>    axprops = dict(xticks=[], yticks=[])<br>    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)<br>    plotTree.totalW = float(getNumLeafs(inTree))<br>    plotTree.totalD = float(getTreeDepth(inTree))<br>    plotTree.xOff = -0.5/plotTree.totalW<br>    plotTree.yOff = 1.0<br>    plotTree(inTree, (0.5,1.0), ‘’)<br>    plt.show()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#10;## &#20915;&#31574;&#26641;(ID3)&#20998;&#31867;&#10;&#20381;&#38752;&#35757;&#32451;&#25968;&#25454;&#26500;&#36896;&#20102;&#20915;&#31574;&#26641;&#20043;&#21518;&#65292;&#21487;&#29992;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#22312;&#25191;&#34892;&#25968;&#25454;&#20998;&#31867;&#26102;&#65292;&#38656;&#35201;&#20915;&#31574;&#26641;&#20197;&#21450;&#29992;&#20110;&#26500;&#36896;&#26641;&#30340;&#26631;&#31614;&#21521;&#37327;&#12290;&#27604;&#36739;&#27979;&#35797;&#25968;&#25454;&#19982;&#20915;&#31574;&#26641;&#19978;&#30340;&#25968;&#20540;&#65292;&#36882;&#24402;&#25191;&#34892;&#30452;&#33267;&#36827;&#20837;&#21494;&#23376;&#33410;&#28857;&#65292;&#20026;&#26368;&#32456;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>def classify(inputTree, featLables, testVec):<br>    firstStr = inputTree.keys()[0]<br>    secondDict = inputTree[firstStr]<br>    featIndex = featLabels.index(firstStr)<br>    for key in secondDict.keys():<br>        if testVec[featIndex] == key:<br>            if type(secondDict[key]).<strong>name</strong> == ‘dict’:<br>                classLabel = classify(secondDict[key], featLabels, testVec)<br>            else:<br>                classLabel = secondDict[key]<br>    return classLabel<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#10;&#26500;&#36896;&#20915;&#31574;&#26641;&#26159;&#24456;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#21487;&#20197;&#21033;&#29992;python&#30340;pickle&#24207;&#21015;&#21270;&#23545;&#35937;&#65292;&#23558;&#20915;&#31574;&#26641;&#20445;&#23384;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>def storeTree(inputTree, filename):<br>    import pickle<br>    fw = open(filename, ‘w’)<br>    pickle.dump(inputTree, fw)<br>    fw.close()</p>
<p>def grabTree(filename):<br>    import pickle<br>    fr = open(filename)<br>    return pickle.load(fr)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#10;## &#31034;&#20363;&#10;&#21033;&#29992;&#20915;&#31574;&#26641;&#24110;&#21161;&#39044;&#27979;&#24739;&#32773;&#38656;&#35201;&#20329;&#25140;&#30340;&#38544;&#24418;&#30524;&#38236;&#31867;&#22411;&#12290;</span><br></pre></td></tr></table></figure></p>
<p>if <strong>name</strong> == ‘<strong>main</strong>‘:<br>    fr = open(‘lenses.txt’)<br>    lenses = [inst.strip().split(‘\t’) for inst in fr.readlines()]<br>    lensesLabels = [‘age’, ‘prescript’, ‘astigmatic’, ‘tearRate’]<br>    lensesTree = createTree(lenses, lensesLabels)<br>    createPlot(lensesTree)<br>```</p>
<p>沿着决策树的不同分支，可以得到不同患者需要佩戴的隐形眼镜类型。从绘制的决策树图可见，医生至多需要问四个问题就可以确认患者需要佩戴的隐形眼镜类型。<br><img src="http://7xqb0e.com1.z0.glb.clouddn.com/decisionTree.png" alt="ID3算法产生的决策树"><br>决策树能够匹配的选项可能太多了，这种问题称为过度匹配(overfitting)。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。如果叶子节点只能增加少许信息，则可以删除该节点，将它并入到其他叶子节点中。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，指导数据集中的所有数据属于同一分类，ID3算法可以用于划分标称型数据。决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题，通过裁剪决策树，合并相邻无法产生大量信息增益的叶节点，消除过度匹配问题。</p>
</div><div class="tags"><a href="/tags/machine-learning/">machine learning</a></div><div class="post-nav"><a href="/2016/02/17/knn/" class="next">K-近邻算法<i class="icon-next"></i></a></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title">分類</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/maching-learning/">maching learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工作/">工作</a></li></ul></div><div class="widget"><div class="widget-title">標籤</div><div class="tagcloud"><a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/ranker/" style="font-size: 15px;">ranker</a> <a href="/tags/预算/" style="font-size: 15px;">预算</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/enum/" style="font-size: 15px;">enum</a></div></div><div class="widget"><div class="widget-title">最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/02/19/tree/">决策树</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/17/knn/">K-近邻算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/enum/">策略枚举</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/26/ranker-online/">ADX 排序模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/20/magina/">实时预算分配</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/04/git/">Git 基本命令</a></li></ul></div><div class="widget"><div class="widget-title">友站連結</div><ul></ul><a href="http://www.joelonsoftware.com/" title="Joe On Software" target="_blank">Joe On Software</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">I See Fire.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>