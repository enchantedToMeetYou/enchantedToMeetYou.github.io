<!DOCTYPE html><html lang="null"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>决策树 | I See Fire</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/pure-min.css"><link rel="stylesheet" type="text/css" href="/css/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">决策树</h1><a id="logo" href="/.">I See Fire</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="icon-home"> 首頁</i></a><a href="/archives/"><i class="icon-archive"> 所有文章</i></a><a href="/about/"><i class="icon-about"> 關於</i></a><a href="/atom.xml"><i class="icon-rss"> 訂閱</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post post-page"><h1 class="post-title">决策树</h1><div class="post-meta">2016-02-19 | <span class="categories">分類於<a href="/categories/maching-learning/"> maching learning</a></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目錄</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#u4FE1_u606F_u589E_u76CA"><span class="toc-number">1.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u5212_u5206_u6570_u636E_u96C6"><span class="toc-number">2.</span> <span class="toc-text">划分数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811"><span class="toc-number">3.</span> <span class="toc-text">递归调用决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u793A_u4F8B"><span class="toc-number">4.</span> <span class="toc-text">示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#u603B_u7ED3"><span class="toc-number">5.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><h2 id="u4FE1_u606F_u589E_u76CA"><a href="#u4FE1_u606F_u589E_u76CA" class="headerlink" title="信息增益"></a>信息增益</h2><p>划分数据集的最大原则是：将无序的数据变的更加有序。组织杂乱无章的数据的一种方法就是使用信息论度量信息。在划分数据集前后发生的信息变化称为信息增益，通过计算每个特征值划分数据集获的信息增益，获得信息增益最高的特征就是最优的选择。<br>如果待分类的事物可能划分在多个分类之中，则符号$x_i$的信息定义为:<br>$$a+b=c$$<br>而熵表示为所有类别可能值包含的信息期望值，定义如下:<br>$$k=m \times n $$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    <span class="keyword">print</span> numEntries</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts:</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">print</span> labelCounts</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * (log(prob)/log(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>
<h2 id="u5212_u5206_u6570_u636E_u96C6"><a href="#u5212_u5206_u6570_u636E_u96C6" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>分类算法除了需要测量信息熵，还需要划分数据集，度量数据集的熵，以便判断是否正确的划分了数据集。需要对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最优的选择。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>])-<span class="number">1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)</span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        <span class="keyword">print</span> infoGain</span><br><span class="line">        <span class="keyword">if</span>(infoGain &gt; bestInfoGain):</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>
<h2 id="u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811"><a href="#u9012_u5F52_u8C03_u7528_u51B3_u7B56_u6811" class="headerlink" title="递归调用决策树"></a>递归调用决策树</h2><p>工作原理：得到原始数据集，基于最好的属性值划分数据集，由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，再次划分数据，因此可以用递归的原则建立决策树。递归终止条件满足:</p>
<ul>
<li>遍历数据集的所有特征属性</li>
<li>每个分支下所有的实例都具有相同的分类</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount:</span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels)</span>:</span></span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    //类别完全相同， 停止继续划分</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    //遍历完所有特征返回出现次数最多的</span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">## Matplotlib绘制决策树</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 决策树(ID3)分类</span></span><br><span class="line">依靠训练数据构造了决策树之后，可用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。比较测试数据与决策树上的数值，递归执行直至进入叶子节点，为最终的分类结果。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree, featLables, testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> testVec[featIndex] == key:</span><br><span class="line">            <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">                classLabel = classify(secondDict[key], featLabels, testVec)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                classLabel = secondDict[key]</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>
<p>构造决策树是很耗时的任务，为了提高效率，可以利用python的pickle序列化对象，将决策树保存。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def storeTree(inputTree, filename):&#10;    import pickle&#10;    fw = open(filename, &#39;w&#39;)&#10;    pickle.dump(inputTree, fw)&#10;    fw.close()&#10;&#10;def grabTree(filename):&#10;    import pickle&#10;    fr = open(filename)&#10;    return pickle.load(fr)</span><br></pre></td></tr></table></figure>
<h2 id="u793A_u4F8B"><a href="#u793A_u4F8B" class="headerlink" title="示例"></a>示例</h2><p>利用决策树帮助预测患者需要佩戴的隐形眼镜类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#39;__main__&#39;:&#10;    fr = open(&#39;lenses.txt&#39;)&#10;    lenses = [inst.strip().split(&#39;\t&#39;) for inst in fr.readlines()]&#10;    lensesLabels = [&#39;age&#39;, &#39;prescript&#39;, &#39;astigmatic&#39;, &#39;tearRate&#39;]&#10;    lensesTree = createTree(lenses, lensesLabels)&#10;    createPlot(lensesTree)</span><br></pre></td></tr></table></figure>
<p>沿着决策树的不同分支，可以得到不同患者需要佩戴的隐形眼镜类型。从绘制的决策树图可见，医生至多需要问四个问题就可以确认患者需要佩戴的隐形眼镜类型。<br><img src="http://7xqb0e.com1.z0.glb.clouddn.com/decisionTree.png" alt="ID3算法产生的决策树"><br>决策树能够匹配的选项可能太多了，这种问题称为过度匹配(overfitting)。为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。如果叶子节点只能增加少许信息，则可以删除该节点，将它并入到其他叶子节点中。</p>
<h2 id="u603B_u7ED3"><a href="#u603B_u7ED3" class="headerlink" title="总结"></a>总结</h2><p>决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，指导数据集中的所有数据属于同一分类，ID3算法可以用于划分标称型数据。决策树可能会产生过多的数据集划分，从而产生过度匹配数据集的问题，通过裁剪决策树，合并相邻无法产生大量信息增益的叶节点，消除过度匹配问题。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt&#10;&#10;decisionNode = dict(boxstyle=&#34;sawtooth&#34;, fc=&#34;0.8&#34;)&#10;leafNode = dict(boxstyle=&#34;round4&#34;, fc=&#34;0.8&#34;)&#10;arrow_args = dict(arrowstyle=&#34;&#60;-&#34;)&#10;&#10;def plotNode(nodeTxt, centerPt, parentPt, nodeType):&#10;    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=&#39;axes fraction&#39;, \&#10;        xytext=centerPt, textcoords=&#39;axes fraction&#39;, va=&#34;center&#34;, ha=&#34;center&#34;, \&#10;        bbox=nodeType, arrowprops=arrow_args)&#10;&#10;def plotMidText(cntrPt, parentPt, txtString):&#10;    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]&#10;    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]&#10;    createPlot.ax1.text(xMid, yMid, txtString)&#10;&#10;def plotTree(myTree, parentPt, nodeTxt):&#10;    numLeafs = getNumLeafs(myTree)&#10;    depth = getTreeDepth(myTree)&#10;    firstStr = myTree.keys()[0]&#10;    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)&#10;    plotMidText(cntrPt, parentPt, nodeTxt)&#10;    plotNode(firstStr, cntrPt, parentPt, decisionNode)&#10;    secondDict = myTree[firstStr]&#10;    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD&#10;    for key in secondDict.keys():&#10;        if type(secondDict[key]).__name__ == &#39;dict&#39;:&#10;            plotTree(secondDict[key], cntrPt, str(key))&#10;        else:&#10;            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW&#10;            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)&#10;            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))&#10;    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD&#10;&#10;def createPlot(inTree):&#10;    fig = plt.figure(1, facecolor=&#39;white&#39;)&#10;    fig.clf()&#10;    axprops = dict(xticks=[], yticks=[])&#10;    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)&#10;    plotTree.totalW = float(getNumLeafs(inTree))&#10;    plotTree.totalD = float(getTreeDepth(inTree))&#10;    plotTree.xOff = -0.5/plotTree.totalW&#10;    plotTree.yOff = 1.0&#10;    plotTree(inTree, (0.5,1.0), &#39;&#39;)&#10;    plt.show()</span><br></pre></td></tr></table></figure>
</div><div class="tags"><a href="/tags/machine-learning/">machine learning</a></div><div class="post-nav"><a href="/2016/02/17/knn/" class="next">K-近邻算法<i class="icon-next"></i></a></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search" class="search-form-input"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title">分類</div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/git/">git</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/maching-learning/">maching learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工作/">工作</a></li></ul></div><div class="widget"><div class="widget-title">標籤</div><div class="tagcloud"><a href="/tags/machine-learning/" style="font-size: 15px;">machine learning</a> <a href="/tags/ranker/" style="font-size: 15px;">ranker</a> <a href="/tags/预算/" style="font-size: 15px;">预算</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/enum/" style="font-size: 15px;">enum</a></div></div><div class="widget"><div class="widget-title">最新文章</div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/02/19/tree/">决策树</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/17/knn/">K-近邻算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/02/01/enum/">策略枚举</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/26/ranker-online/">ADX 排序模块</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/20/magina/">实时预算分配</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/01/04/git/">Git 基本命令</a></li></ul></div><div class="widget"><div class="widget-title">友站連結</div><ul></ul><a href="http://www.joelonsoftware.com/" title="Joe On Software" target="_blank">Joe On Software</a></div></div></div></div><div id="footer">© <a href="/." rel="nofollow">I See Fire.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div><a id="rocket" href="#top" class="show"></a><script src="/js/jquery.min.js" type="text/javascript"></script>
<script src="/js/totop.js" type="text/javascript"></script><script src="/js/fancybox.pack.js" type="text/javascript"></script>
<script src="/js/jquery.fancybox.js" type="text/javascript"></script><link rel="stylesheet" href="/css/jquery.fancybox.css" type="text/css"></div><!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body></html>